{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from decimal import Decimal\n",
    "from scipy import stats\n",
    "from statsmodels.sandbox.stats import multicomp\n",
    "\n",
    "# import utils\n",
    "sys.path.append(\"../../utils\")\n",
    "from plotting_utils import *\n",
    "from misc_utils import *\n",
    "from norm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_snps(wt_df, index_elem_df):\n",
    "    snp_map = {}\n",
    "    \n",
    "    for i, row in wt_df.iterrows():\n",
    "        wt_id = row.unique_id\n",
    "        tile_name = row.tile_name\n",
    "        snp_df = index_elem_df[(index_elem_df[\"tile_name\"] == tile_name) & \n",
    "                               (~index_elem_df[\"oligo_type\"].str.contains(\"DELETION\")) &\n",
    "                               (~index_elem_df[\"oligo_type\"].str.contains(\"SCRAMBLED\"))]\n",
    "        snp_names = list(snp_df[snp_df[\"SNP\"] != \"none\"][\"SNP\"])\n",
    "        snp_ids = list(snp_df[snp_df[\"SNP\"] != \"none\"][\"unique_id\"])\n",
    "        n_snps = len(snp_names)\n",
    "        snp_map[wt_id] = [snp_ids, snp_names]\n",
    "    return snp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_snp_data(snp_map, data, pvals, activ_colname, padj_colname, l2fc_colname, barcode_thresh, activity_pval_thresh, active_l2fc_thresh, repressive_l2fc_thresh, score_type):\n",
    "    \n",
    "    snp_pvals = {}\n",
    "    snp_l2fc = {}\n",
    "\n",
    "    for wt_id in snp_map.keys():\n",
    "        snp_names = snp_map[wt_id][1]\n",
    "        snp_ids = snp_map[wt_id][0]\n",
    "        \n",
    "        for i in range(0, len(snp_names)):\n",
    "            snp_name = snp_names[i]\n",
    "            snp_id = snp_ids[i]\n",
    "\n",
    "            # get values to do test\n",
    "            wt_vals = list(data[data[\"unique_id\"] == wt_id][activ_colname])\n",
    "            snp_vals = list(data[data[\"unique_id\"] == snp_id][activ_colname])\n",
    "\n",
    "            # count non-NANs in values\n",
    "            wt_non_nans = len(wt_vals) - np.sum(np.isnan(wt_vals))\n",
    "            snp_non_nans = len(snp_vals) - np.sum(np.isnan(snp_vals))\n",
    "\n",
    "            # find mean val of wt & snp\n",
    "            wt_median = np.nanmedian(wt_vals)\n",
    "            snp_median = np.nanmedian(snp_vals)\n",
    "            l2fc = snp_median - wt_median\n",
    "\n",
    "            # get wildtype/SNP tilepval\n",
    "            try:\n",
    "                wt_pval = pvals[pvals[\"unique_id\"] == wt_id][padj_colname].iloc[0]\n",
    "                snp_pval = pvals[pvals[\"unique_id\"] == snp_id][padj_colname].iloc[0]\n",
    "                wt_activ = pvals[pvals[\"unique_id\"] == wt_id][l2fc_colname].iloc[0]\n",
    "                snp_activ = pvals[pvals[\"unique_id\"] == snp_id][l2fc_colname].iloc[0]\n",
    "            except IndexError:\n",
    "                wt_pval = 1\n",
    "                snp_pval = 1\n",
    "                wt_activ = np.nan\n",
    "                snp_activ = np.nan\n",
    "\n",
    "            if wt_pval < activity_pval_thresh:\n",
    "                if wt_activ > active_l2fc_thresh:\n",
    "                    wt_status = \"sig active\"\n",
    "                elif wt_activ < repressive_l2fc_thresh:\n",
    "                    wt_status = \"sig repressive\"\n",
    "                else:\n",
    "                    wt_status = \"not sig\"\n",
    "            else:\n",
    "                wt_status = \"not sig\"\n",
    "\n",
    "            if snp_pval < activity_pval_thresh:\n",
    "                if snp_activ > active_l2fc_thresh:\n",
    "                    snp_status = \"sig active\"\n",
    "                elif snp_activ < repressive_l2fc_thresh:\n",
    "                    snp_status = \"sig repressive\"\n",
    "                else:\n",
    "                    snp_status = \"not sig\"\n",
    "            else:\n",
    "                snp_status = \"not sig\"\n",
    "\n",
    "            # to calculate SNP/WT pval, require at least 20 barcodes in each. \n",
    "            # also require either the WT or SNP tile to be significantly active (or significantly repressive)\n",
    "            # else \"NA\"    \n",
    "            if score_type == \"active\":\n",
    "                if wt_status == \"sig active\" or snp_status == \"sig active\":\n",
    "                    u, pval = stats.mannwhitneyu(wt_vals, snp_vals, alternative=\"two-sided\", use_continuity=False)\n",
    "                    snp_pvals[snp_id] = pval\n",
    "                else:\n",
    "                    snp_pvals[snp_id] = \"NA__no_active_tile\"\n",
    "            \n",
    "            if score_type == \"repressive\":\n",
    "                if wt_status == \"sig repressive\" or snp_status == \"sig repressive\":\n",
    "                    u, pval = stats.mannwhitneyu(wt_vals, snp_vals, alternative=\"two-sided\", use_continuity=False)\n",
    "                    snp_pvals[snp_id] = pval\n",
    "                else:\n",
    "                    snp_pvals[snp_id] = \"NA__no_repressive_tile\"\n",
    "                    \n",
    "            if wt_non_nans < barcode_thresh or snp_non_nans < barcode_thresh:\n",
    "                snp_pvals[snp_id] = \"NA__not_enough_barcodes\"\n",
    "\n",
    "            snp_l2fc[snp_id] = [wt_median, snp_median, l2fc, wt_id]\n",
    "\n",
    "    return snp_pvals, snp_l2fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_padj(row, rep):\n",
    "    if not pd.isnull(row[\"%s_padj_indiv\" % rep]):\n",
    "        return row[\"%s_padj_indiv\" % rep]\n",
    "    elif not pd.isnull(row[\"%s_padj_haplo\" % rep]):\n",
    "        return row[\"%s_padj_haplo\" % rep]\n",
    "    else:\n",
    "        return row[\"%s_pval\" % rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_pvals(row, cols):\n",
    "    pvals = list(row[cols])\n",
    "    non_na_pvals = [x for x in pvals if \"NA\" not in (str(x))]\n",
    "    if len(non_na_pvals) > 0:\n",
    "        new_pval = stats.combine_pvalues(non_na_pvals, method=\"stouffer\")[1]\n",
    "    else:\n",
    "        new_pval = \"NA__too_many_rep_NAs\"\n",
    "    return new_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_pvals(reps, snp_map, data, pvals, min_barcodes, activ_alpha, active_l2fc_thresh, repr_l2fc_thresh, score_type):\n",
    "    print(\"%s SNPs, reps: %s\" % (score_type, reps))\n",
    "    for i, rep in enumerate(reps):\n",
    "        print(\"...%s pvals...\" % (rep))\n",
    "        snp_pvals, snp_l2fc = get_snp_data(snp_map, data, pvals, rep, \"%s_padj\" % rep,\n",
    "                                           \"%s_log2fc\" % rep, min_barcodes, activ_alpha, \n",
    "                                           active_l2fc_thresh, repr_l2fc_thresh, score_type)\n",
    "        print(len(snp_pvals))\n",
    "\n",
    "        tmp1 = pd.DataFrame.from_dict(snp_pvals, orient=\"index\").reset_index()\n",
    "        tmp1.columns = [\"unique_id\", \"%s_pval\" % rep]\n",
    "        tmp2 = pd.DataFrame.from_dict(snp_l2fc, orient=\"index\").reset_index()\n",
    "        tmp2.columns = [\"unique_id\", \"%s_wt_med\" % rep, \"%s_snp_med\" % rep, \"%s_l2fc\" % rep, \"wt_id\"]\n",
    "        if i == 0:\n",
    "            tmp = tmp1.merge(tmp2, on=\"unique_id\")\n",
    "            snp_data = tmp.copy()\n",
    "        else:\n",
    "            tmp2.drop(\"wt_id\", axis=1, inplace=True)\n",
    "            tmp = tmp1.merge(tmp2, on=\"unique_id\")\n",
    "            snp_data = snp_data.merge(tmp, on=\"unique_id\")\n",
    "        print(len(snp_data))\n",
    "    return snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_df_and_correct(rep, snp_data, pool_type, colname):\n",
    "    pval_col = \"%s_pval\" % rep\n",
    "\n",
    "    # first individual\n",
    "    data_nonan_indiv = snp_data[(~snp_data[pval_col].astype(str).str.contains(\"NA\")) & \n",
    "                                (~snp_data[\"unique_id\"].str.contains(\"HAPLO\"))][[\"unique_id\", \n",
    "                                                                                 \"wt_id\", \n",
    "                                                                                 pval_col]].drop_duplicates()\n",
    "    try:\n",
    "        data_nonan_indiv[\"%s_padj_indiv\" % rep] = multicomp.multipletests(data_nonan_indiv[pval_col], \n",
    "                                                                          method=\"bonferroni\")[1]\n",
    "    except ZeroDivisionError:\n",
    "        data_nonan_indiv[\"%s_padj_indiv\" % rep] = np.nan\n",
    "\n",
    "    # then haplotypes\n",
    "    if pool_type == \"POOL1\":\n",
    "        data_nonan_haplo = snp_data[(~snp_data[pval_col].astype(str).str.contains(\"NA\")) & \n",
    "                                    (snp_data[\"unique_id\"].str.contains(\"HAPLO\"))][[\"unique_id\", \n",
    "                                                                                    \"wt_id\", \n",
    "                                                                                    pval_col]].drop_duplicates()\n",
    "        \n",
    "        try:\n",
    "            data_nonan_haplo[\"%s_padj_haplo\" % rep] = multicomp.multipletests(data_nonan_haplo[pval_col], \n",
    "                                                                              method=\"bonferroni\")[1]\n",
    "        except ZeroDivisionError:\n",
    "            data_nonan_indiv[\"%s_padj_haplo\" % rep] = np.nan\n",
    "\n",
    "    # join back together\n",
    "    snp_data = snp_data.merge(data_nonan_indiv, on=[\"unique_id\", \"wt_id\", pval_col], how=\"left\")\n",
    "    if pool_type == \"POOL1\":\n",
    "        snp_data = snp_data.merge(data_nonan_haplo, on=[\"unique_id\", \"wt_id\", pval_col], how=\"left\")\n",
    "    #print(len(snp_data))\n",
    "\n",
    "    # pick either indiv or haplo pval\n",
    "    if pool_type == \"POOL1\":\n",
    "        snp_data[colname] = snp_data.apply(pick_padj, rep=rep, axis=1)\n",
    "        snp_data.drop([\"%s_padj_indiv\" % rep, \"%s_padj_haplo\" % rep], inplace=True, axis=1)\n",
    "    else:\n",
    "        snp_data[colname] = snp_data[\"%s_padj_indiv\" % rep]\n",
    "        snp_data.drop([\"%s_padj_indiv\" % rep], inplace=True, axis=1)\n",
    "    return snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correct for multiple testing within each replicate\n",
    "def correct_pvals(reps, snp_data, pool_type, combined, colprefix):\n",
    "    if combined:\n",
    "        rep = colprefix + \"combined\"\n",
    "        colname = colprefix + \"combined_padj\"\n",
    "        snp_data = split_df_and_correct(rep, snp_data, pool_type, colname)\n",
    "    else:\n",
    "        for rep in reps:\n",
    "            rep = colprefix + rep\n",
    "            colname = colprefix + \"%s_padj\" % rep\n",
    "            snp_data = split_df_and_correct(rep, snp_data, pool_type, colname)\n",
    "\n",
    "    return snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_pval(row):\n",
    "    argmax = row[\"argmax_val\"]\n",
    "    try:\n",
    "        samp = argmax.split(\"_\")[1]\n",
    "        col = \"samp_%s_combined_pval\" % samp\n",
    "        val = row[col]\n",
    "    except:\n",
    "        return np.nan\n",
    "    return val\n",
    "\n",
    "def get_max_padj(row):\n",
    "    argmax = row[\"argmax_val\"]\n",
    "    try:\n",
    "        samp = argmax.split(\"_\")[1]\n",
    "        col = \"samp_%s_combined_padj\" % samp\n",
    "        val = row[col]\n",
    "    except:\n",
    "        return np.nan\n",
    "    return val\n",
    "\n",
    "def get_max_l2fc(row):\n",
    "    argmax = row[\"argmax_val\"]\n",
    "    try:\n",
    "        samp = argmax.split(\"_\")[1]\n",
    "        col = \"samp_%s_combined_l2fc\" % samp\n",
    "        val = row[col]\n",
    "    except:\n",
    "        return np.nan\n",
    "    return val\n",
    "\n",
    "def get_max_wt_med(row):\n",
    "    argmax = row[\"argmax_val\"]\n",
    "    try:\n",
    "        samp = argmax.split(\"_\")[1]\n",
    "        col = \"samp_%s_combined_wt_med\" % samp\n",
    "        val = row[col]\n",
    "    except:\n",
    "        return np.nan\n",
    "    return val\n",
    "\n",
    "def get_max_snp_med(row):\n",
    "    argmax = row[\"argmax_val\"]\n",
    "    try:\n",
    "        samp = argmax.split(\"_\")[1]\n",
    "        col = \"samp_%s_combined_snp_med\" % samp\n",
    "        val = row[col]\n",
    "    except:\n",
    "        return np.nan\n",
    "    return val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter - require the same direction in at least 75% of non-na replicates\n",
    "def sig_status(row, col, thresh, l2fc_cols):\n",
    "    if \"NA\" in str(row[col]) or pd.isnull(row[col]):\n",
    "        return \"NA__too_many_rep_NAs\"\n",
    "    elif row[col] < thresh:\n",
    "        l2fcs = list(row[l2fc_cols])\n",
    "        neg = [x for x in l2fcs if x < 0]\n",
    "        pos = [x for x in l2fcs if x > 0]\n",
    "        perc_neg = len(neg)/float(len(neg)+len(pos))\n",
    "        perc_pos = len(pos)/float(len(neg)+len(pos))\n",
    "        if perc_neg > 0.75 or perc_pos > 0.75:\n",
    "            return \"sig\"\n",
    "        else:\n",
    "            return \"not sig\"\n",
    "    else:\n",
    "        return \"not sig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sig_from_samples(row, samp_combined_cols, alpha):\n",
    "    vals = list(row[samp_combined_cols])\n",
    "    vals = [float(x) for x in vals if \"NA\" not in str(x)]\n",
    "    return len([x for x in vals if x < alpha])\n",
    "\n",
    "def define_samples_as_sig(row, n_samples):\n",
    "    if row[\"n_sig_samples\"] == 0:\n",
    "        if \"NA\" in str(row[\"combined_padj\"]):\n",
    "            return \"NA__too_many_rep_NAs\"\n",
    "        else:\n",
    "            return \"not sig\"\n",
    "    elif row[\"n_sig_samples\"] >= 0.75 * n_samples:\n",
    "        return \"sig\"\n",
    "    else:\n",
    "        return \"not sig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_snp_results(reps, snp_map, data, pvals, min_barcodes, activ_alpha, active_l2fc_thresh, repr_l2fc_thresh, \n",
    "                    score_type, pool_type, n_samples):\n",
    "\n",
    "    snp_data = calculate_pvals(reps, snp_map, data, pvals, min_barcodes, activ_alpha, active_l2fc_thresh, \n",
    "                               repr_l2fc_thresh, score_type)\n",
    "    snp_data = correct_pvals(reps, snp_data, pool_type, False, \"\")\n",
    "    \n",
    "    rep_pvals = [x + \"_pval\" for x in reps]\n",
    "    rep_padjs = [x + \"_padj\" for x in reps]\n",
    "    rep_l2fcs = [x + \"_l2fc\" for x in reps]\n",
    "    rep_wt_meds = [x + \"_wt_med\" for x in reps]\n",
    "    rep_snp_meds = [x + \"_snp_med\" for x in reps]\n",
    "    \n",
    "    snp_data[\"combined_pval\"] = snp_data.apply(combine_pvals, cols=rep_pvals, axis=1)\n",
    "    snp_data[\"combined_l2fc\"] = snp_data[rep_l2fcs].mean(axis=1)\n",
    "    snp_data[\"combined_wt_med\"] = snp_data[rep_wt_meds].mean(axis=1)\n",
    "    snp_data[\"combined_snp_med\"] = snp_data[rep_snp_meds].mean(axis=1)\n",
    "    \n",
    "    snp_data = correct_pvals(reps, snp_data, pool_type, True, \"\")\n",
    "    \n",
    "    for n in range(n_samples):\n",
    "        sampled_reps = list(np.random.choice(reps, size=4))\n",
    "\n",
    "        rep_pvals = [x + \"_pval\" for x in sampled_reps]\n",
    "        rep_padjs = [x + \"_padj\" for x in sampled_reps]\n",
    "        rep_l2fcs = [x + \"_l2fc\" for x in sampled_reps]\n",
    "        rep_wt_meds = [x + \"_wt_med\" for x in sampled_reps]\n",
    "        rep_snp_meds = [x + \"_snp_med\" for x in sampled_reps]\n",
    "\n",
    "        snp_data[\"samp_%s_combined_pval\" % n] = snp_data.apply(combine_pvals, cols=rep_pvals, axis=1)\n",
    "        snp_data[\"samp_%s_combined_l2fc\" % n] = snp_data[rep_l2fcs].mean(axis=1)\n",
    "        snp_data[\"samp_%s_combined_wt_med\" % n] = snp_data[rep_wt_meds].mean(axis=1)\n",
    "        snp_data[\"samp_%s_combined_snp_med\" % n] = snp_data[rep_snp_meds].mean(axis=1)\n",
    "\n",
    "        snp_data = correct_pvals(reps, snp_data, pool_type, True, \"samp_%s_\" % n)\n",
    "\n",
    "    # see how many of the sampled combined p-vals are < alpha\n",
    "    samp_combined_cols = [x for x in snp_data.columns if \"samp\" in x and \"combined_padj\" in x]\n",
    "    print(samp_combined_cols)\n",
    "    snp_data[\"n_sig_samples\"] = snp_data.apply(count_sig_from_samples, \n",
    "                                               samp_combined_cols=samp_combined_cols, \n",
    "                                               alpha=0.05, axis=1)\n",
    "\n",
    "    # consider significant if this # is >= 75% of samples\n",
    "    snp_data[\"downsamp_sig\"] = snp_data.apply(define_samples_as_sig, n_samples=n_samples, axis=1)\n",
    "\n",
    "    to_drop = [x for x in snp_data.columns if x.startswith(\"samp_\")]\n",
    "    snp_data.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wt_or_snp(row):\n",
    "    if row.SNP == \"none\":\n",
    "        return \"ref\"\n",
    "    else:\n",
    "        return \"alt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snp_type(row, col):\n",
    "    if row[col] == \"not sig\":\n",
    "        return \"not sig\"\n",
    "    elif row[col] == \"sig\":\n",
    "        if \"CONTROL\"in row[\"unique_id\"]:\n",
    "            return \"sig control\"\n",
    "        elif \"HAPLO\" in row[\"unique_id\"]:\n",
    "            return \"sig haplo\"\n",
    "        else:\n",
    "            return \"sig indiv\"\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_snp_names(row, name_dict, loc_dict):\n",
    "    old_name = row[\"wt_id\"]\n",
    "    chrom = old_name.split(\"__\")[3].split(\":\")[0]\n",
    "    start = int(old_name.split(\"__\")[3].split(\":\")[1].split(\"..\")[0])\n",
    "    end = int(old_name.split(\"__\")[3].split(\":\")[1].split(\"..\")[1].split(\",\")[0])\n",
    "    strand = old_name.split(\"__\")[3].split(\",\")[1]\n",
    "    locs = \"%s:%s-%s\" % (chrom, start, end)\n",
    "    if strand == \"+\":\n",
    "        text_strand = \"plus\"\n",
    "    else:\n",
    "        text_strand = \"minus\"\n",
    "    tile_num = int(old_name.split(\"__\")[4].split(\".\")[1])\n",
    "    \n",
    "    name = old_name.split(\"__\")[2]\n",
    "    coords = old_name.split(\"__\")[3].split(\",\")[0]\n",
    "    try:\n",
    "        gene = name.split(\",\")[0].split(\"@\")[1]\n",
    "        prom = name.split(\",\")[0].split(\"@\")[0]\n",
    "    except:\n",
    "        gene = \"X\"\n",
    "        prom = \"pX\"\n",
    "    \n",
    "    if gene not in name_dict.keys() and coords not in loc_dict.keys():\n",
    "        name = \"%s__%s__tile%s\" % (gene, prom, tile_num)\n",
    "    elif gene in name_dict.keys():\n",
    "        name = \"%s__%s__tile%s\" % (name_dict[gene], prom, tile_num)\n",
    "    elif coords in loc_dict.keys():\n",
    "        name = \"%s__%s__tile%s\" % (loc_dict[coords], prom, tile_num)\n",
    "    \n",
    "    clean_name = \"%s__%s\" % (name, text_strand)\n",
    "    return clean_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_snp_pos(row, wt_seqs_dict, snp_seqs_dict, max_snps_per_tile):\n",
    "    wt_id = row[\"wt_id\"]\n",
    "    snp_id = row[\"unique_id\"]\n",
    "    wt_seq = wt_seqs_dict[wt_id]\n",
    "    snp_seq = snp_seqs_dict[snp_id]\n",
    "    try:\n",
    "        pos = [i for i in range(len(wt_seq)) if wt_seq[i] != snp_seq[i]][0]\n",
    "    except:\n",
    "        pos = [i for i in range(len(snp_seq)) if wt_seq[i] != snp_seq[i]][0]\n",
    "    pos = pos-10\n",
    "    return pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
